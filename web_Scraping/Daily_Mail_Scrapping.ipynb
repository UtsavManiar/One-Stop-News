{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping: Daily Mail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Elapsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_news_dailymail():\n",
    "    \n",
    "    # url definition\n",
    "    url = \"https://www.dailymail.co.uk/sport/index.html\"\n",
    "    \n",
    "    # Request\n",
    "    r1 = requests.get(url)\n",
    "    r1.status_code\n",
    "\n",
    "    # We'll save in coverpage the cover page content\n",
    "    coverpage = r1.content\n",
    "\n",
    "    # Soup creation\n",
    "    soup1 = BeautifulSoup(coverpage, 'html5lib')\n",
    "\n",
    "    # News identification\n",
    "    coverpage_news = soup1.find_all('h2', class_='linkro-darkred')\n",
    "    len(coverpage_news)\n",
    "    \n",
    "    number_of_articles = 5\n",
    "\n",
    "    # Empty lists for content, links and titles\n",
    "    news_contents = []\n",
    "    list_links = []\n",
    "    list_titles = []\n",
    "\n",
    "    for n in np.arange(0, number_of_articles):\n",
    "\n",
    "        # Getting the link of the article\n",
    "        link = url + coverpage_news[n].find('a')['href']\n",
    "        list_links.append(link)\n",
    "\n",
    "        # Getting the title\n",
    "        title = coverpage_news[n].find('a').get_text()\n",
    "        list_titles.append(title)\n",
    "\n",
    "        # Reading the content (it is divided in paragraphs)\n",
    "        article = requests.get(link)\n",
    "        article_content = article.content\n",
    "        soup_article = BeautifulSoup(article_content, 'html5lib')\n",
    "        body = soup_article.find_all('p', class_='mol-para-with-font')\n",
    "\n",
    "        # Unifying the paragraphs\n",
    "        list_paragraphs = []\n",
    "        final_article = ''\n",
    "        for p in np.arange(0, len(body)):\n",
    "            paragraph = body[p].get_text()\n",
    "            list_paragraphs.append(paragraph)\n",
    "            final_article = \" \".join(list_paragraphs)\n",
    "\n",
    "         # Removing special characters\n",
    "        final_article = re.sub(\"\\\\xa0\", \"\", final_article)\n",
    "        \n",
    "        news_contents.append(final_article)\n",
    "        \n",
    "\n",
    "    # df_features\n",
    "    df_features = pd.DataFrame(\n",
    "         {'Content': news_contents \n",
    "        })\n",
    "\n",
    "    # df_show_info\n",
    "    df_show_info = pd.DataFrame(\n",
    "        {'Article Title': list_titles,\n",
    "         'Article Link': list_links,\n",
    "         'Newspaper': 'Daily Mail'})\n",
    "    \n",
    "    return (df_features, df_show_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The time elapsed is 4.196715 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "x, y = get_news_dailymail()\n",
    "end =time.time()\n",
    "te = end-start\n",
    "print(\"The time elapsed is %f seconds\" %(te))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrapping output for sports page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article Title</th>\n",
       "      <th>Article Link</th>\n",
       "      <th>Newspaper</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Arsenal manager Mikel Arteta tests POSITIVE fo...</td>\n",
       "      <td>https://www.dailymail.co.uk/sport/index.html/s...</td>\n",
       "      <td>Daily Mail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Premier League to hold EMERGENCY MEETING on Fr...</td>\n",
       "      <td>https://www.dailymail.co.uk/sport/index.html/s...</td>\n",
       "      <td>Daily Mail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Chelsea winger Callum Hudson-Odoi tests positi...</td>\n",
       "      <td>https://www.dailymail.co.uk/sport/index.html/s...</td>\n",
       "      <td>Daily Mail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>'Bruno Fernandes is on fire... every time he g...</td>\n",
       "      <td>https://www.dailymail.co.uk/sport/index.html/s...</td>\n",
       "      <td>Daily Mail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Chelsea's Champions League trip to Bayern Muni...</td>\n",
       "      <td>https://www.dailymail.co.uk/sport/index.html/s...</td>\n",
       "      <td>Daily Mail</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Article Title  \\\n",
       "0  Arsenal manager Mikel Arteta tests POSITIVE fo...   \n",
       "1  Premier League to hold EMERGENCY MEETING on Fr...   \n",
       "2  Chelsea winger Callum Hudson-Odoi tests positi...   \n",
       "3  'Bruno Fernandes is on fire... every time he g...   \n",
       "4  Chelsea's Champions League trip to Bayern Muni...   \n",
       "\n",
       "                                        Article Link   Newspaper  \n",
       "0  https://www.dailymail.co.uk/sport/index.html/s...  Daily Mail  \n",
       "1  https://www.dailymail.co.uk/sport/index.html/s...  Daily Mail  \n",
       "2  https://www.dailymail.co.uk/sport/index.html/s...  Daily Mail  \n",
       "3  https://www.dailymail.co.uk/sport/index.html/s...  Daily Mail  \n",
       "4  https://www.dailymail.co.uk/sport/index.html/s...  Daily Mail  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_news_dailymail():\n",
    "    \n",
    "    # url definition\n",
    "    url = \"https://www.dailymail.co.uk\"\n",
    "    \n",
    "    # Request\n",
    "    r1 = requests.get(url)\n",
    "    r1.status_code\n",
    "\n",
    "    # We'll save in coverpage the cover page content\n",
    "    coverpage = r1.content\n",
    "\n",
    "    # Soup creation\n",
    "    soup1 = BeautifulSoup(coverpage, 'html5lib')\n",
    "\n",
    "    # News identification\n",
    "    coverpage_news = soup1.find_all('h2', class_='linkro-darkred')\n",
    "    len(coverpage_news)\n",
    "    \n",
    "    number_of_articles = 5\n",
    "\n",
    "    # Empty lists for content, links and titles\n",
    "    news_contents = []\n",
    "    list_links = []\n",
    "    list_titles = []\n",
    "\n",
    "    for n in np.arange(0, number_of_articles):\n",
    "\n",
    "        # Getting the link of the article\n",
    "        link = url + coverpage_news[n].find('a')['href']\n",
    "        list_links.append(link)\n",
    "\n",
    "        # Getting the title\n",
    "        title = coverpage_news[n].find('a').get_text()\n",
    "        list_titles.append(title)\n",
    "\n",
    "        # Reading the content (it is divided in paragraphs)\n",
    "        article = requests.get(link)\n",
    "        article_content = article.content\n",
    "        soup_article = BeautifulSoup(article_content, 'html5lib')\n",
    "        body = soup_article.find_all('p', class_='mol-para-with-font')\n",
    "\n",
    "        # Unifying the paragraphs\n",
    "        list_paragraphs = []\n",
    "        final_article = ''\n",
    "        for p in np.arange(0, len(body)):\n",
    "            paragraph = body[p].get_text()\n",
    "            list_paragraphs.append(paragraph)\n",
    "            final_article = \" \".join(list_paragraphs)\n",
    "\n",
    "         # Removing special characters\n",
    "        final_article = re.sub(\"\\\\xa0\", \"\", final_article)\n",
    "        \n",
    "        news_contents.append(final_article)\n",
    "        \n",
    "\n",
    "    # df_features\n",
    "    df_features = pd.DataFrame(\n",
    "         {'Content': news_contents \n",
    "        })\n",
    "\n",
    "    # df_show_info\n",
    "    df_show_info = pd.DataFrame(\n",
    "        {'Article Title': list_titles,\n",
    "         'Article Link': list_links,\n",
    "         'Newspaper': 'Daily Mail'})\n",
    "    \n",
    "    return (df_features, df_show_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The time elapsed is 16.501077 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "x, y = get_news_dailymail()\n",
    "end =time.time()\n",
    "te = end-start\n",
    "print(\"The time elapsed is %f seconds\" %(te))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrapping output for headlines page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article Title</th>\n",
       "      <th>Article Link</th>\n",
       "      <th>Newspaper</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Coronavirus pandemonium in New York: Manhattan...</td>\n",
       "      <td>https://www.dailymail.co.uk/news/article-81065...</td>\n",
       "      <td>Daily Mail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Canadian Prime Minister Justin Trudeau's wife ...</td>\n",
       "      <td>https://www.dailymail.co.uk/news/article-81075...</td>\n",
       "      <td>Daily Mail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>'We're losing time': Doctor warns of 'tragic s...</td>\n",
       "      <td>https://www.dailymail.co.uk/news/article-81057...</td>\n",
       "      <td>Daily Mail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Chinese spokesman accuses the US military of b...</td>\n",
       "      <td>https://www.dailymail.co.uk/news/article-81055...</td>\n",
       "      <td>Daily Mail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>America's coronavirus response is FAILING, adm...</td>\n",
       "      <td>https://www.dailymail.co.uk/news/article-81055...</td>\n",
       "      <td>Daily Mail</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Article Title  \\\n",
       "0  Coronavirus pandemonium in New York: Manhattan...   \n",
       "1  Canadian Prime Minister Justin Trudeau's wife ...   \n",
       "2  'We're losing time': Doctor warns of 'tragic s...   \n",
       "3  Chinese spokesman accuses the US military of b...   \n",
       "4  America's coronavirus response is FAILING, adm...   \n",
       "\n",
       "                                        Article Link   Newspaper  \n",
       "0  https://www.dailymail.co.uk/news/article-81065...  Daily Mail  \n",
       "1  https://www.dailymail.co.uk/news/article-81075...  Daily Mail  \n",
       "2  https://www.dailymail.co.uk/news/article-81057...  Daily Mail  \n",
       "3  https://www.dailymail.co.uk/news/article-81055...  Daily Mail  \n",
       "4  https://www.dailymail.co.uk/news/article-81055...  Daily Mail  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
