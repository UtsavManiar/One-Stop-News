{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('articles_bbc_2018_01_30.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(309, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(308, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of                                               articles lang\n",
       "0    Image copyright PA/EPA Image caption Oligarch ...   en\n",
       "1    Husband admits killing French jogger\\r\\n\\r\\nTh...   en\n",
       "2    Media playback is unsupported on your device M...   en\n",
       "3    Manchester City's Leroy Sane is ruled out for ...   en\n",
       "4    Image copyright AFP Image caption Sebastien Br...   en\n",
       "..                                                 ...  ...\n",
       "303  فيديو\\r\\n\\r\\nكيف تعبر الحدود...مثل الفيل؟!\\r\\n...   ar\n",
       "304  بالصور\\r\\n\\r\\nمعالم لندن تحت الأضواء\\r\\n\\r\\nمع...   ar\n",
       "305  يقدم لكم تلفزيون بي بي سي العربي الأخبار والأخ...   ar\n",
       "306  موجات FM\\r\\n\\r\\nنبث إرسالنا على موجات إف إم في...   ar\n",
       "307  Hi I am the head of product for BBC News Onlin...   en\n",
       "\n",
       "[308 rows x 2 columns]>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Keeping English articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['lang'] = data.articles.apply(detect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "en    257\n",
       "fa      9\n",
       "fr      7\n",
       "id      5\n",
       "uk      4\n",
       "ar      4\n",
       "vi      4\n",
       "hi      4\n",
       "ru      4\n",
       "sw      3\n",
       "tr      2\n",
       "pt      2\n",
       "es      2\n",
       "de      1\n",
       "Name: lang, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.lang.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.loc[data.lang=='en']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Image copyright PA/EPA Image caption Oligarch Roman Abramovich (l) and PM Dmitry Medvedev are on the list\\r\\n\\r\\nRussian President Vladimir Putin says a list of officials and businessmen close to the Kremlin published by the US has in effect targeted all Russian people.',\n",
       " 'The list names 210 top Russians as part of a sanctions law aimed at punishing Moscow for meddling in the US election.',\n",
       " 'However, the US stressed those named were not subject to new sanctions.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['sentences'] = data.articles.apply(sent_tokenize)\n",
    "data['sentences'].head(1).tolist()[0][:3] # Print the first 3 sentences of the 1st article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Image', 'copyright', 'PA/EPA', 'Image', 'caption', 'Oligarch', 'Roman', 'Abramovich', '(', 'l', ')', 'and', 'PM', 'Dmitry', 'Medvedev', 'are', 'on', 'the', 'list', 'Russian', 'President', 'Vladimir', 'Putin', 'says', 'a', 'list', 'of', 'officials', 'and', 'businessmen', 'close', 'to', 'the', 'Kremlin', 'published', 'by', 'the', 'US', 'has', 'in', 'effect', 'targeted', 'all', 'Russian', 'people', '.'], ['The', 'list', 'names', '210', 'top', 'Russians', 'as', 'part', 'of', 'a', 'sanctions', 'law', 'aimed', 'at', 'punishing', 'Moscow', 'for', 'meddling', 'in', 'the', 'US', 'election', '.'], ['However', ',', 'the', 'US', 'stressed', 'those', 'named', 'were', 'not', 'subject', 'to', 'new', 'sanctions', '.']]\n"
     ]
    }
   ],
   "source": [
    "data['tokens_sentences'] = data['sentences'].apply(lambda sentences: [word_tokenize(sentence) for sentence in sentences])\n",
    "print(data['tokens_sentences'].head(1).tolist()[0][:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatizing with POS tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('Image', 'NN'), ('copyright', 'NN'), ('PA/EPA', 'NNP'), ('Image', 'NNP'), ('caption', 'NN'), ('Oligarch', 'NNP'), ('Roman', 'NNP'), ('Abramovich', 'NNP'), ('(', '('), ('l', 'NN'), (')', ')'), ('and', 'CC'), ('PM', 'NNP'), ('Dmitry', 'NNP'), ('Medvedev', 'NNP'), ('are', 'VBP'), ('on', 'IN'), ('the', 'DT'), ('list', 'NN'), ('Russian', 'NNP'), ('President', 'NNP'), ('Vladimir', 'NNP'), ('Putin', 'NNP'), ('says', 'VBZ'), ('a', 'DT'), ('list', 'NN'), ('of', 'IN'), ('officials', 'NNS'), ('and', 'CC'), ('businessmen', 'NNS'), ('close', 'RB'), ('to', 'TO'), ('the', 'DT'), ('Kremlin', 'NNP'), ('published', 'VBN'), ('by', 'IN'), ('the', 'DT'), ('US', 'NNP'), ('has', 'VBZ'), ('in', 'IN'), ('effect', 'NN'), ('targeted', 'VBN'), ('all', 'DT'), ('Russian', 'JJ'), ('people', 'NNS'), ('.', '.')], [('The', 'DT'), ('list', 'NN'), ('names', 'RB'), ('210', 'CD'), ('top', 'JJ'), ('Russians', 'NNPS'), ('as', 'IN'), ('part', 'NN'), ('of', 'IN'), ('a', 'DT'), ('sanctions', 'NNS'), ('law', 'NN'), ('aimed', 'VBN'), ('at', 'IN'), ('punishing', 'VBG'), ('Moscow', 'NNP'), ('for', 'IN'), ('meddling', 'VBG'), ('in', 'IN'), ('the', 'DT'), ('US', 'NNP'), ('election', 'NN'), ('.', '.')], [('However', 'RB'), (',', ','), ('the', 'DT'), ('US', 'NNP'), ('stressed', 'VBD'), ('those', 'DT'), ('named', 'VBN'), ('were', 'VBD'), ('not', 'RB'), ('subject', 'JJ'), ('to', 'TO'), ('new', 'JJ'), ('sanctions', 'NNS'), ('.', '.')]]\n"
     ]
    }
   ],
   "source": [
    "data['POS_tokens'] = data['tokens_sentences'].apply(lambda tokens_sentences: [pos_tag(tokens) for tokens in tokens_sentences])\n",
    "print(data['POS_tokens'].head(1).tolist()[0][:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatizing each word with its POS tag, in each sentence\n",
    "data['tokens_sentences_lemmatized'] = data['POS_tokens'].apply(\n",
    "    lambda list_tokens_POS: [\n",
    "        [\n",
    "            lemmatizer.lemmatize(el[0], get_wordnet_pos(el[1])) \n",
    "            if get_wordnet_pos(el[1]) != '' else el[0] for el in tokens_POS\n",
    "        ] \n",
    "        for tokens_POS in list_tokens_POS\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Image',\n",
       "  'copyright',\n",
       "  'PA/EPA',\n",
       "  'Image',\n",
       "  'caption',\n",
       "  'Oligarch',\n",
       "  'Roman',\n",
       "  'Abramovich',\n",
       "  '(',\n",
       "  'l',\n",
       "  ')',\n",
       "  'and',\n",
       "  'PM',\n",
       "  'Dmitry',\n",
       "  'Medvedev',\n",
       "  'be',\n",
       "  'on',\n",
       "  'the',\n",
       "  'list',\n",
       "  'Russian',\n",
       "  'President',\n",
       "  'Vladimir',\n",
       "  'Putin',\n",
       "  'say',\n",
       "  'a',\n",
       "  'list',\n",
       "  'of',\n",
       "  'official',\n",
       "  'and',\n",
       "  'businessmen',\n",
       "  'close',\n",
       "  'to',\n",
       "  'the',\n",
       "  'Kremlin',\n",
       "  'publish',\n",
       "  'by',\n",
       "  'the',\n",
       "  'US',\n",
       "  'have',\n",
       "  'in',\n",
       "  'effect',\n",
       "  'target',\n",
       "  'all',\n",
       "  'Russian',\n",
       "  'people',\n",
       "  '.'],\n",
       " ['The',\n",
       "  'list',\n",
       "  'names',\n",
       "  '210',\n",
       "  'top',\n",
       "  'Russians',\n",
       "  'as',\n",
       "  'part',\n",
       "  'of',\n",
       "  'a',\n",
       "  'sanction',\n",
       "  'law',\n",
       "  'aim',\n",
       "  'at',\n",
       "  'punish',\n",
       "  'Moscow',\n",
       "  'for',\n",
       "  'meddle',\n",
       "  'in',\n",
       "  'the',\n",
       "  'US',\n",
       "  'election',\n",
       "  '.'],\n",
       " ['However',\n",
       "  ',',\n",
       "  'the',\n",
       "  'US',\n",
       "  'stress',\n",
       "  'those',\n",
       "  'name',\n",
       "  'be',\n",
       "  'not',\n",
       "  'subject',\n",
       "  'to',\n",
       "  'new',\n",
       "  'sanction',\n",
       "  '.']]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['tokens_sentences_lemmatized'].head(1).tolist()[0][:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regrouping tokens and removing stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords_verbs = ['say', 'get', 'go', 'know', 'may', 'need', 'like', 'make', 'see', 'want', 'come', 'take', 'use', 'would', 'can']\n",
    "stopwords_other = ['one', 'mr', 'bbc', 'image', 'getty', 'de', 'en', 'caption', 'also', 'copyright', 'something']\n",
    "my_stopwords = stopwords.words('English') + stopwords_verbs + stopwords_other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain # to flatten list of sentences of tokens into list of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tokens'] = data['tokens_sentences_lemmatized'].map(lambda sentences: list(chain.from_iterable(sentences)))\n",
    "data['tokens'] = data['tokens'].map(lambda tokens: [token.lower() for token in tokens if token.isalpha() \n",
    "                                                    and token.lower() not in my_stopwords and len(token)>1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['oligarch',\n",
       " 'roman',\n",
       " 'abramovich',\n",
       " 'pm',\n",
       " 'dmitry',\n",
       " 'medvedev',\n",
       " 'list',\n",
       " 'russian',\n",
       " 'president',\n",
       " 'vladimir',\n",
       " 'putin',\n",
       " 'list',\n",
       " 'official',\n",
       " 'businessmen',\n",
       " 'close',\n",
       " 'kremlin',\n",
       " 'publish',\n",
       " 'us',\n",
       " 'effect',\n",
       " 'target',\n",
       " 'russian',\n",
       " 'people',\n",
       " 'list',\n",
       " 'names',\n",
       " 'top',\n",
       " 'russians',\n",
       " 'part',\n",
       " 'sanction',\n",
       " 'law',\n",
       " 'aim']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['tokens'].head(1).tolist()[0][:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare bi-grams and tri-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = data['tokens'].tolist()\n",
    "bigram_model = Phrases(tokens)\n",
    "trigram_model = Phrases(bigram_model[tokens], min_count=1)\n",
    "tokens = list(trigram_model[bigram_model[tokens]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare objects for LDA gensim implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_LDA = corpora.Dictionary(tokens)\n",
    "dictionary_LDA.filter_extremes(no_below=3)\n",
    "corpus = [dictionary_LDA.doc2bow(tok) for tok in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import models\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.06 s\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(123456)\n",
    "num_topics = 20\n",
    "%time lda_model = models.LdaModel(corpus, num_topics=num_topics, \\\n",
    "                                  id2word=dictionary_LDA, \\\n",
    "                                  passes=4, alpha=[0.01]*num_topics, \\\n",
    "                                  eta=[0.01]*len(dictionary_LDA.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick exploration of LDA results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Looking at topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 0.012*\"specie\" + 0.010*\"prey\" + 0.007*\"act\" + 0.006*\"animal\" + 0.006*\"help\" + 0.005*\"back\" + 0.005*\"find\" + 0.005*\"become\" + 0.005*\"behaviour\" + 0.005*\"number\" + 0.005*\"area\" + 0.005*\"include\" + 0.004*\"order\" + 0.004*\"kill\" + 0.004*\"approach\" + 0.004*\"however\" + 0.004*\"move\" + 0.004*\"host\" + 0.004*\"evolve\" + 0.004*\"well\"\n",
      "\n",
      "1: 0.010*\"show\" + 0.008*\"game\" + 0.007*\"another\" + 0.007*\"light\" + 0.007*\"night\" + 0.006*\"find\" + 0.006*\"could\" + 0.006*\"predator\" + 0.006*\"animal\" + 0.006*\"give\" + 0.006*\"images\" + 0.006*\"question\" + 0.006*\"however\" + 0.006*\"group\" + 0.006*\"live\" + 0.006*\"time\" + 0.005*\"transparent\" + 0.005*\"drug\" + 0.005*\"eye\" + 0.005*\"call\"\n",
      "\n",
      "2: 0.006*\"first\" + 0.006*\"people\" + 0.006*\"work\" + 0.005*\"city\" + 0.005*\"time\" + 0.005*\"year\" + 0.004*\"day\" + 0.004*\"part\" + 0.004*\"find\" + 0.004*\"could\" + 0.004*\"us\" + 0.003*\"run\" + 0.003*\"restaurant\" + 0.003*\"plan\" + 0.003*\"home\" + 0.003*\"start\" + 0.003*\"tunnel\" + 0.003*\"include\" + 0.003*\"bring\" + 0.003*\"dish\"\n",
      "\n",
      "3: 0.014*\"separatist\" + 0.011*\"people\" + 0.008*\"die\" + 0.008*\"call\" + 0.007*\"coalition\" + 0.007*\"northern_mali\" + 0.007*\"ansar_dine\" + 0.007*\"city\" + 0.006*\"force\" + 0.006*\"play\" + 0.006*\"islamist\" + 0.005*\"government\" + 0.005*\"around\" + 0.005*\"victim\" + 0.005*\"steal\" + 0.005*\"talk\" + 0.005*\"year\" + 0.005*\"prime_minister\" + 0.005*\"show\" + 0.005*\"add\"\n",
      "\n",
      "4: 0.008*\"people\" + 0.007*\"wave\" + 0.006*\"tell\" + 0.006*\"town\" + 0.006*\"water\" + 0.006*\"ocean\" + 0.005*\"benefit\" + 0.005*\"village\" + 0.005*\"world\" + 0.005*\"back\" + 0.005*\"leave\" + 0.004*\"us\" + 0.004*\"time\" + 0.004*\"fin\" + 0.004*\"work\" + 0.004*\"man\" + 0.004*\"look\" + 0.004*\"way\" + 0.004*\"first\" + 0.004*\"city\"\n",
      "\n",
      "5: 0.022*\"dutch\" + 0.011*\"issue\" + 0.010*\"border\" + 0.010*\"news\" + 0.009*\"become\" + 0.009*\"israel\" + 0.008*\"netherlands\" + 0.008*\"agency\" + 0.008*\"story\" + 0.007*\"report\" + 0.007*\"service\" + 0.007*\"un\" + 0.007*\"several\" + 0.006*\"international\" + 0.006*\"might\" + 0.006*\"part\" + 0.006*\"shy\" + 0.005*\"output\" + 0.005*\"problem\" + 0.005*\"local\"\n",
      "\n",
      "6: 0.012*\"event\" + 0.011*\"people\" + 0.009*\"president\" + 0.009*\"digital\" + 0.008*\"authority\" + 0.008*\"show\" + 0.007*\"news\" + 0.007*\"end\" + 0.007*\"election\" + 0.007*\"supporter\" + 0.007*\"correspondent\" + 0.006*\"capital\" + 0.006*\"grow\" + 0.006*\"still\" + 0.006*\"report\" + 0.006*\"well\" + 0.006*\"leave\" + 0.006*\"national\" + 0.006*\"film\" + 0.005*\"programme\"\n",
      "\n",
      "7: 0.011*\"news\" + 0.010*\"ant\" + 0.008*\"twitter\" + 0.007*\"single\" + 0.007*\"live\" + 0.006*\"name\" + 0.006*\"specie\" + 0.006*\"small\" + 0.006*\"people\" + 0.006*\"story\" + 0.005*\"welcome\" + 0.005*\"mate\" + 0.005*\"effort\" + 0.005*\"alert\" + 0.005*\"receive\" + 0.005*\"power\" + 0.005*\"millipede\" + 0.005*\"consider\" + 0.005*\"pollution\" + 0.005*\"tiny\"\n",
      "\n",
      "8: 0.012*\"us\" + 0.008*\"russia\" + 0.007*\"could\" + 0.007*\"time\" + 0.007*\"write\" + 0.006*\"president\" + 0.006*\"rocket\" + 0.006*\"call\" + 0.006*\"satellite\" + 0.006*\"trump\" + 0.005*\"many\" + 0.005*\"people\" + 0.005*\"raise\" + 0.005*\"flight\" + 0.005*\"set\" + 0.005*\"political\" + 0.005*\"tell\" + 0.004*\"try\" + 0.004*\"job\" + 0.004*\"mission\"\n",
      "\n",
      "9: 0.014*\"case\" + 0.010*\"male\" + 0.009*\"review\" + 0.009*\"prey\" + 0.008*\"find\" + 0.007*\"evidence\" + 0.007*\"kick\" + 0.007*\"message\" + 0.007*\"live\" + 0.006*\"life\" + 0.006*\"place\" + 0.006*\"study\" + 0.006*\"predator\" + 0.006*\"give\" + 0.006*\"part\" + 0.006*\"people\" + 0.006*\"number\" + 0.005*\"mate\" + 0.005*\"put\" + 0.004*\"store\"\n",
      "\n",
      "10: 0.009*\"people\" + 0.006*\"government\" + 0.006*\"uk\" + 0.006*\"think\" + 0.006*\"brexit\" + 0.005*\"tell\" + 0.005*\"time\" + 0.005*\"could\" + 0.005*\"us\" + 0.005*\"work\" + 0.004*\"even\" + 0.004*\"find\" + 0.004*\"minister\" + 0.004*\"day\" + 0.004*\"already\" + 0.004*\"help\" + 0.004*\"test\" + 0.004*\"bee\" + 0.004*\"still\" + 0.004*\"back\"\n",
      "\n",
      "11: 0.015*\"us\" + 0.014*\"stress\" + 0.011*\"healthcare\" + 0.008*\"day\" + 0.008*\"sleep\" + 0.007*\"could\" + 0.007*\"president\" + 0.007*\"test\" + 0.007*\"find\" + 0.007*\"keep\" + 0.006*\"put\" + 0.006*\"try\" + 0.006*\"images\" + 0.006*\"amazon\" + 0.006*\"provide\" + 0.006*\"team\" + 0.005*\"benefit\" + 0.005*\"expert\" + 0.005*\"body\" + 0.005*\"meeting\"\n",
      "\n",
      "12: 0.017*\"work\" + 0.013*\"people\" + 0.011*\"call\" + 0.008*\"model\" + 0.007*\"help\" + 0.007*\"find\" + 0.007*\"woman\" + 0.006*\"sleep\" + 0.006*\"spanish\" + 0.006*\"age\" + 0.006*\"radio\" + 0.006*\"happen\" + 0.006*\"report\" + 0.005*\"well\" + 0.005*\"add\" + 0.005*\"include\" + 0.005*\"accord\" + 0.005*\"police\" + 0.005*\"much\" + 0.005*\"job\"\n",
      "\n",
      "13: 0.034*\"video\" + 0.021*\"logo\" + 0.020*\"design\" + 0.015*\"platform\" + 0.014*\"tourism\" + 0.012*\"app\" + 0.012*\"animal\" + 0.012*\"country\" + 0.011*\"respect\" + 0.010*\"new\" + 0.009*\"include\" + 0.009*\"device\" + 0.009*\"call\" + 0.007*\"content\" + 0.007*\"around\" + 0.006*\"home\" + 0.006*\"others\" + 0.006*\"key\" + 0.006*\"television\" + 0.006*\"news_app\"\n",
      "\n",
      "14: 0.017*\"us\" + 0.007*\"time\" + 0.006*\"first\" + 0.006*\"america\" + 0.006*\"year\" + 0.006*\"product\" + 0.005*\"find\" + 0.005*\"amazon\" + 0.005*\"company\" + 0.005*\"still\" + 0.005*\"call\" + 0.005*\"president\" + 0.004*\"customer\" + 0.004*\"live\" + 0.004*\"store\" + 0.004*\"trump\" + 0.004*\"even\" + 0.004*\"put\" + 0.004*\"back\" + 0.004*\"light\"\n",
      "\n",
      "15: 0.014*\"list\" + 0.012*\"data\" + 0.012*\"us\" + 0.011*\"publish\" + 0.010*\"name\" + 0.010*\"could\" + 0.010*\"base\" + 0.009*\"study\" + 0.009*\"human\" + 0.007*\"russian\" + 0.007*\"security\" + 0.006*\"work\" + 0.006*\"journalist\" + 0.006*\"report\" + 0.006*\"activity\" + 0.005*\"level\" + 0.005*\"think\" + 0.005*\"system\" + 0.005*\"add\" + 0.005*\"afghanistan\"\n",
      "\n",
      "16: 0.009*\"find\" + 0.008*\"light\" + 0.008*\"us\" + 0.007*\"people\" + 0.006*\"work\" + 0.006*\"help\" + 0.005*\"day\" + 0.005*\"much\" + 0.005*\"think\" + 0.005*\"year\" + 0.005*\"images\" + 0.005*\"first\" + 0.005*\"actor\" + 0.005*\"write\" + 0.005*\"africa\" + 0.005*\"company\" + 0.004*\"right\" + 0.004*\"feel\" + 0.004*\"give\" + 0.004*\"week\"\n",
      "\n",
      "17: 0.010*\"idea\" + 0.008*\"work\" + 0.008*\"link\" + 0.007*\"news\" + 0.007*\"find\" + 0.007*\"world\" + 0.007*\"fast\" + 0.006*\"place\" + 0.006*\"information\" + 0.005*\"note\" + 0.005*\"start\" + 0.005*\"content\" + 0.005*\"chris\" + 0.005*\"audience\" + 0.005*\"human\" + 0.005*\"body\" + 0.005*\"include\" + 0.004*\"pick\" + 0.004*\"material\" + 0.004*\"journalism\"\n",
      "\n",
      "18: 0.020*\"eu\" + 0.008*\"tell\" + 0.008*\"government\" + 0.007*\"europe\" + 0.007*\"country\" + 0.006*\"visit\" + 0.006*\"include\" + 0.006*\"brexit\" + 0.006*\"work\" + 0.006*\"day\" + 0.006*\"new\" + 0.006*\"flight\" + 0.005*\"travel\" + 0.005*\"two\" + 0.005*\"deal\" + 0.005*\"vision\" + 0.005*\"couple\" + 0.005*\"reuters\" + 0.004*\"part\" + 0.004*\"route\"\n",
      "\n",
      "19: 0.026*\"video\" + 0.019*\"http\" + 0.017*\"drama\" + 0.013*\"set\" + 0.013*\"code\" + 0.011*\"always\" + 0.011*\"reference\" + 0.010*\"thank\" + 0.010*\"music\" + 0.009*\"listen\" + 0.009*\"young\" + 0.009*\"decade\" + 0.008*\"could\" + 0.008*\"live\" + 0.008*\"think\" + 0.008*\"end\" + 0.008*\"state\" + 0.008*\"love\" + 0.008*\"show\" + 0.008*\"receive\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i,topic in lda_model.show_topics(formatted=True, num_topics=num_topics, num_words=20):\n",
    "    print(str(i)+\": \"+ topic)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Allocating topics to documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image copyright PA/EPA Image caption Oligarch Roman Abramovich (l) and PM Dmitry Medvedev are on the list\r\n",
      "\r\n",
      "Russian President Vladimir Putin says a list of officials and businessmen close to the Kremlin published by the US has in effect targeted all Russian people.\r\n",
      "\r\n",
      "The list names 210 top Russians as part of a sanctions law aimed at punishing Moscow for meddling in the US election.\r\n",
      "\r\n",
      "However, the US stressed those named were not subject to new sanctions.\r\n",
      "\r\n",
      "Mr Putin said the list was an unfr\n"
     ]
    }
   ],
   "source": [
    "print(data.articles.loc[0][:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(15, 0.99830645)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model[corpus[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting topics on unseen document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Topic # : 7, Probability : 0.84, Topics : 0.011*\"news\" + 0.010*\"ant\" + 0.008*\"twitter\" + 0.007*\"single\" + 0.007*\"live\" + 0.006*\"name\" + 0.006*\"specie\" + 0.006*\"small\" + 0.006*\"people\" + 0.006*\"story\" + 0.005*\"welcome\" + 0.005*\"mate\" + 0.005*\"effort\" + 0.005*\"alert\" + 0.005*\"receive\" + 0.005*\"power\" + 0.005*\"millipede\" + 0.005*\"consider\" + 0.005*\"pollution\" + 0.005*\"tiny\"']\n"
     ]
    }
   ],
   "source": [
    "document = 'open(\"MyFile.txt\",\"a\")'\n",
    "tokens = word_tokenize(document)\n",
    "topics = lda_model.show_topics(formatted=True, num_topics=num_topics, num_words=20)\n",
    "print([('Topic # : %s, Probability : %s, Topics : %s')%(el[0], round(el[1],2), topics[el[0]][1]) for el in lda_model[dictionary_LDA.doc2bow(tokens)]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "widgets": {
   "state": {
    "429f605e71e34a868915456ee4fb20b8": {
     "views": [
      {
       "cell_index": 14
      }
     ]
    },
    "46817b7b86ca4b65a4bd631d2e7d777f": {
     "views": [
      {
       "cell_index": 21
      }
     ]
    },
    "8559963a3ac94b19a5b3ea6214032316": {
     "views": [
      {
       "cell_index": 19
      }
     ]
    },
    "96323249f1884bd5b62f87c3145932df": {
     "views": [
      {
       "cell_index": 8
      }
     ]
    },
    "d9d5e9ab22a947f398aad3ed9c365fcf": {
     "views": [
      {
       "cell_index": 16
      }
     ]
    },
    "fa80f472c5f14bcd9a7ba0befad06400": {
     "views": [
      {
       "cell_index": 9
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
