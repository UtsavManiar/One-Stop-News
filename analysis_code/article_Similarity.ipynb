{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langdetect import detect\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from scipy import spatial\n",
    "from itertools import chain\n",
    "from nltk.corpus import stopwords\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('articles_bbc_2018_01_30.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(309, 2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(308, 2)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.dropna().reset_index(drop=True)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of                                               articles lang\n",
       "0    Image copyright PA/EPA Image caption Oligarch ...   en\n",
       "1    Husband admits killing French jogger\\r\\n\\r\\nTh...   en\n",
       "2    Media playback is unsupported on your device M...   en\n",
       "3    Manchester City's Leroy Sane is ruled out for ...   en\n",
       "4    Image copyright AFP Image caption Sebastien Br...   en\n",
       "..                                                 ...  ...\n",
       "303  فيديو\\r\\n\\r\\nكيف تعبر الحدود...مثل الفيل؟!\\r\\n...   ar\n",
       "304  بالصور\\r\\n\\r\\nمعالم لندن تحت الأضواء\\r\\n\\r\\nمع...   ar\n",
       "305  يقدم لكم تلفزيون بي بي سي العربي الأخبار والأخ...   ar\n",
       "306  موجات FM\\r\\n\\r\\nنبث إرسالنا على موجات إف إم في...   ar\n",
       "307  Hi I am the head of product for BBC News Onlin...   en\n",
       "\n",
       "[308 rows x 2 columns]>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "#### Keeping English articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['lang'] = data.articles.apply(detect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "en    257\n",
       "fa      9\n",
       "fr      7\n",
       "id      5\n",
       "hi      4\n",
       "ru      4\n",
       "ar      4\n",
       "vi      4\n",
       "uk      4\n",
       "sw      3\n",
       "es      2\n",
       "pt      2\n",
       "tr      2\n",
       "de      1\n",
       "Name: lang, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.lang.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.loc[data.lang=='en']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Image copyright PA/EPA Image caption Oligarch Roman Abramovich (l) and PM Dmitry Medvedev are on the list\\r\\n\\r\\nRussian President Vladimir Putin says a list of officials and businessmen close to the Kremlin published by the US has in effect targeted all Russian people.',\n",
       " 'The list names 210 top Russians as part of a sanctions law aimed at punishing Moscow for meddling in the US election.',\n",
       " 'However, the US stressed those named were not subject to new sanctions.']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['sentences'] = data.articles.apply(sent_tokenize)\n",
    "data['sentences'].head(1).tolist()[0][:3] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Image', 'copyright', 'PA/EPA', 'Image', 'caption', 'Oligarch', 'Roman', 'Abramovich', '(', 'l', ')', 'and', 'PM', 'Dmitry', 'Medvedev', 'are', 'on', 'the', 'list', 'Russian', 'President', 'Vladimir', 'Putin', 'says', 'a', 'list', 'of', 'officials', 'and', 'businessmen', 'close', 'to', 'the', 'Kremlin', 'published', 'by', 'the', 'US', 'has', 'in', 'effect', 'targeted', 'all', 'Russian', 'people', '.'], ['The', 'list', 'names', '210', 'top', 'Russians', 'as', 'part', 'of', 'a', 'sanctions', 'law', 'aimed', 'at', 'punishing', 'Moscow', 'for', 'meddling', 'in', 'the', 'US', 'election', '.'], ['However', ',', 'the', 'US', 'stressed', 'those', 'named', 'were', 'not', 'subject', 'to', 'new', 'sanctions', '.']]\n"
     ]
    }
   ],
   "source": [
    "data['tokens_sentences'] = data['sentences'].apply(lambda sentences: [word_tokenize(sentence) for sentence in sentences])\n",
    "print(data['tokens_sentences'].head(1).tolist()[0][:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatizing with POS tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('Image', 'NN'), ('copyright', 'NN'), ('PA/EPA', 'NNP'), ('Image', 'NNP'), ('caption', 'NN'), ('Oligarch', 'NNP'), ('Roman', 'NNP'), ('Abramovich', 'NNP'), ('(', '('), ('l', 'NN'), (')', ')'), ('and', 'CC'), ('PM', 'NNP'), ('Dmitry', 'NNP'), ('Medvedev', 'NNP'), ('are', 'VBP'), ('on', 'IN'), ('the', 'DT'), ('list', 'NN'), ('Russian', 'NNP'), ('President', 'NNP'), ('Vladimir', 'NNP'), ('Putin', 'NNP'), ('says', 'VBZ'), ('a', 'DT'), ('list', 'NN'), ('of', 'IN'), ('officials', 'NNS'), ('and', 'CC'), ('businessmen', 'NNS'), ('close', 'RB'), ('to', 'TO'), ('the', 'DT'), ('Kremlin', 'NNP'), ('published', 'VBN'), ('by', 'IN'), ('the', 'DT'), ('US', 'NNP'), ('has', 'VBZ'), ('in', 'IN'), ('effect', 'NN'), ('targeted', 'VBN'), ('all', 'DT'), ('Russian', 'JJ'), ('people', 'NNS'), ('.', '.')], [('The', 'DT'), ('list', 'NN'), ('names', 'RB'), ('210', 'CD'), ('top', 'JJ'), ('Russians', 'NNPS'), ('as', 'IN'), ('part', 'NN'), ('of', 'IN'), ('a', 'DT'), ('sanctions', 'NNS'), ('law', 'NN'), ('aimed', 'VBN'), ('at', 'IN'), ('punishing', 'VBG'), ('Moscow', 'NNP'), ('for', 'IN'), ('meddling', 'VBG'), ('in', 'IN'), ('the', 'DT'), ('US', 'NNP'), ('election', 'NN'), ('.', '.')], [('However', 'RB'), (',', ','), ('the', 'DT'), ('US', 'NNP'), ('stressed', 'VBD'), ('those', 'DT'), ('named', 'VBN'), ('were', 'VBD'), ('not', 'RB'), ('subject', 'JJ'), ('to', 'TO'), ('new', 'JJ'), ('sanctions', 'NNS'), ('.', '.')]]\n"
     ]
    }
   ],
   "source": [
    "data['POS_tokens'] = data['tokens_sentences'].apply(lambda tokens_sentences: [pos_tag(tokens) for tokens in tokens_sentences])\n",
    "print(data['POS_tokens'].head(1).tolist()[0][:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatizing each word with its POS tag, in each sentence\n",
    "data['tokens_sentences_lemmatized'] = data['POS_tokens'].apply(\n",
    "    lambda list_tokens_POS: [\n",
    "        [\n",
    "            lemmatizer.lemmatize(el[0], get_wordnet_pos(el[1])) \n",
    "            if get_wordnet_pos(el[1]) != '' else el[0] for el in tokens_POS\n",
    "        ] \n",
    "        for tokens_POS in list_tokens_POS\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Image',\n",
       "  'copyright',\n",
       "  'PA/EPA',\n",
       "  'Image',\n",
       "  'caption',\n",
       "  'Oligarch',\n",
       "  'Roman',\n",
       "  'Abramovich',\n",
       "  '(',\n",
       "  'l',\n",
       "  ')',\n",
       "  'and',\n",
       "  'PM',\n",
       "  'Dmitry',\n",
       "  'Medvedev',\n",
       "  'be',\n",
       "  'on',\n",
       "  'the',\n",
       "  'list',\n",
       "  'Russian',\n",
       "  'President',\n",
       "  'Vladimir',\n",
       "  'Putin',\n",
       "  'say',\n",
       "  'a',\n",
       "  'list',\n",
       "  'of',\n",
       "  'official',\n",
       "  'and',\n",
       "  'businessmen',\n",
       "  'close',\n",
       "  'to',\n",
       "  'the',\n",
       "  'Kremlin',\n",
       "  'publish',\n",
       "  'by',\n",
       "  'the',\n",
       "  'US',\n",
       "  'have',\n",
       "  'in',\n",
       "  'effect',\n",
       "  'target',\n",
       "  'all',\n",
       "  'Russian',\n",
       "  'people',\n",
       "  '.'],\n",
       " ['The',\n",
       "  'list',\n",
       "  'names',\n",
       "  '210',\n",
       "  'top',\n",
       "  'Russians',\n",
       "  'as',\n",
       "  'part',\n",
       "  'of',\n",
       "  'a',\n",
       "  'sanction',\n",
       "  'law',\n",
       "  'aim',\n",
       "  'at',\n",
       "  'punish',\n",
       "  'Moscow',\n",
       "  'for',\n",
       "  'meddle',\n",
       "  'in',\n",
       "  'the',\n",
       "  'US',\n",
       "  'election',\n",
       "  '.'],\n",
       " ['However',\n",
       "  ',',\n",
       "  'the',\n",
       "  'US',\n",
       "  'stress',\n",
       "  'those',\n",
       "  'name',\n",
       "  'be',\n",
       "  'not',\n",
       "  'subject',\n",
       "  'to',\n",
       "  'new',\n",
       "  'sanction',\n",
       "  '.']]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['tokens_sentences_lemmatized'].head(1).tolist()[0][:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regrouping tokens and removing stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_verbs = ['say', 'get', 'go', 'know', 'may', 'need', 'like', 'make', 'see', 'want', 'come', 'take', 'use', 'would', 'can']\n",
    "stopwords_other = ['one', 'mr', 'bbc', 'image', 'getty', 'de', 'en', 'caption', 'also', 'copyright', 'something']\n",
    "my_stopwords = stopwords.words('English') + stopwords_verbs + stopwords_other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tokens'] = data['tokens_sentences_lemmatized'].map(lambda sentences: list(chain.from_iterable(sentences)))\n",
    "data['tokens'] = data['tokens'].map(lambda tokens: [token.lower() for token in tokens if token.isalpha() \n",
    "                                                    and token.lower() not in my_stopwords and len(token)>1])\n",
    "tokens = data['tokens'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus(fname, tokens_only=False):\n",
    "    for i, line in enumerate(fname):\n",
    "            #print(line,i)\n",
    "            yield gensim.models.doc2vec.TaggedDocument(line, [i])\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus = list(read_corpus(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=20,alpha=0.025, min_alpha=0.00025,min_count=2, epochs=20)\n",
    "model.build_vocab(train_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9957830905914307\n"
     ]
    }
   ],
   "source": [
    "article1= open('ar3.txt')\n",
    "first_text =article1.read()\n",
    "article2 = open('ar4.txt')\n",
    "second_text=article2.read()\n",
    "#print(first_text)\n",
    "f_tokens = gensim.utils.simple_preprocess(first_text)\n",
    "s_tokens = gensim.utils.simple_preprocess(second_text)\n",
    "\n",
    "#generate tokens\n",
    "vec1 = model.infer_vector(f_tokens)#(test_corpus[44])\n",
    "vec2 = model.infer_vector(s_tokens)#(train_corpus[209].words)\n",
    "\n",
    "#find cosine similarity\n",
    "similairty = spatial.distance.cosine(vec1, vec2)\n",
    "print(1-similairty)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
